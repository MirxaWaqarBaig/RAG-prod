import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Dict, Any
import json
import time

class LocalQwenLLM:
    """Local Qwen LLM wrapper using Unsloth's 4-bit quantized model"""
    
    def __init__(self):
        print("ğŸ¤– Loading Local Qwen LLM (unsloth/Qwen3-14B-bnb-4bit)...")
        start_time = time.time()
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained("unsloth/Qwen3-14B-bnb-4bit")
        
        # Load model with 4-bit quantization for RTX 4090 compatibility
        self.model = AutoModelForCausalLM.from_pretrained(
            "unsloth/Qwen3-14B-bnb-4bit",
            torch_dtype="auto",
            device_map="auto"
        )
        
        load_time = time.time() - start_time
        print(f"âœ… Local Qwen LLM loaded in {load_time:.2f} seconds")
    
    def complete(self, prompt: str) -> str:
        """Complete a prompt using the local Qwen model"""
        
        # Prepare the model input with chat template
        messages = [{"role": "user", "content": prompt}]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=True  # Enable thinking capability for better reasoning
        )
        
        # Tokenize input
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=2048,  # Reasonable limit for entity extraction
                temperature=0.1,      # Low temperature for consistent output
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Decode response
        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
        
        # Parse thinking content if available
        try:
            # Find </think> token (151668)
            index = len(output_ids) - output_ids[::-1].index(151668)
            thinking_content = self.tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")
            content = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")
        except ValueError:
            # No thinking content found
            thinking_content = ""
            content = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip("\n")
        
        # Return the final content (after thinking)
        return content
    
    def extract_entities_and_relationships(self, document_text: str) -> Dict[str, Any]:
        """Extract entities and relationships using local Qwen model"""
        
        prompt = f"""
è¯·ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{document_text}

è¯·æå–ï¼š
1. å®ä½“ï¼ˆentitiesï¼‰ï¼šåŒ…æ‹¬å…¬å¸ã€è§’è‰²ã€å¹³å°ã€è½¦è¾†ã€æµç¨‹ã€æ–‡æ¡£ã€åœ°ç‚¹ã€åŠŸèƒ½ã€è¦æ±‚ã€æ­¥éª¤ç­‰
2. å…³ç³»ï¼ˆrelationshipsï¼‰ï¼šå®ä½“ä¹‹é—´çš„å…³ç³»ï¼Œå¦‚REQUIRESã€PROVIDESã€BELONGS_TOã€WORKS_FORã€HAS_STEPSã€COMPETES_WITHã€EARNã€LOCATED_INç­‰

è¿”å›æ ¼å¼ï¼š
{{
    "entities": [
        {{"name": "å®ä½“åç§°", "type": "å®ä½“ç±»å‹", "properties": {{"key": "value"}}}}
    ],
    "relationships": [
        {{"source": "æºå®ä½“", "target": "ç›®æ ‡å®ä½“", "type": "å…³ç³»ç±»å‹", "properties": {{"key": "value"}}}}
    ]
}}

è¯·ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°ã€‚
"""
        
        try:
            response = self.complete(prompt)
            
            # Clean response - remove markdown code blocks if present
            response = response.strip()
            if response.startswith("```json"):
                response = response[7:]  # Remove ```json
            if response.startswith("```"):
                response = response[3:]   # Remove ```
            if response.endswith("```"):
                response = response[:-3]  # Remove ```
            
            response = response.strip()
            
            # Parse JSON response
            result = json.loads(response)
            return result
            
        except Exception as e:
            print(f"Error in local LLM graph generation: {e}")
            return {"entities": [], "relationships": []}
